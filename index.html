<!DOCTYPE hvtml>
<!-- saved from url=(0033)https://leiyi420.github.io/glow-wavegan2/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
<title>UniSyn: An end-to-end unified model for text-to-speech and singing voice synthesis</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="TODO: title">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://leiyi420.github.io/glow-wavegan2/">
<meta property="og:url" content="https://leiyi420.github.io/glow-wavegan2/">
<meta name="twitter:card" content="summary">
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="style.css">
  </head>
  <body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
    <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->
      
      
    </section>

<section class="main-content">
    <h1 id=""><center>UniSyn: An end-to-end unified model for text-to-speech and singing voice synthesis</center></h1>
	<!--
    <center> Yi Lei<sup>1</sup>, Shan Yang<sup>2</sup>, Xinsheng Wang<sup>1</sup>, Jixun Yao<sup>1</sup>, Qicong Xie<sup>1</sup>, Lei Xie<sup>1</sup>, Dan Su<sup>2</sup> </center>
    <center> <sup>1</sup> Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xi'an, China</center>
    <center> <sup>2</sup> Tencent AI Lab, China</center>
	-->
<h2>Contents</h2>
<ul>
  <li><a href="#abstract">Abstract</a></li>
  <li><a href="#tts">Demos -- Coming soon...</a></li>
</ul>

<h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
<p>Text-to-speech (TTS) and singing voice synthesis (SVS) aim at generating high-quality speaking and singing voice according to textual input and music scores, respectively. Unifying TTS and SVS into a single system is crucial to the applications requiring both of them. Existing methods usually suffer from some limitations, which rely on either both singing and speaking data from the same person or cascaded models of multiple tasks. To address these problems, a simplified elegant framework for TTS and SVS, named \textit{UniSyn}, is proposed in this paper. It is an end-to-end unified model that can make a voice speak and sing with only singing or speaking data from this person.
 To be specific, a multi-conditional variational autoencoder (MC-VAE), which constructs two independent latent sub-spaces with the speaker- and style-related (i.e. speak or sing) conditions for flexible control, is proposed in UniSyn. Moreover, supervised guided-VAE and timbre perturbation with the Wasserstein distance constraint are adopted to further disentangle the speaker timbre and style.
 Experiments conducted on two speakers and two singers demonstrate that UniSyn can generate natural speaking and singing voice without corresponding training data. The proposed approach outperforms the state-of-the-art end-to-end voice generation work, which proves the effectiveness and advantages of UniSyn.
</p>
	
<br><br>

<h2>Demos -- Coming soon...<a name="tts"></a></h2>

      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub Pages</a>.</span>
      </footer>
    </section>
</body></html>
